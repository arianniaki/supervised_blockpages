\documentclass{article} % For LaTeX2e
\newcommand{\argmin}{\arg\!\min}
\usepackage{mathtools}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{epstopdf} %%package to overcome problem with eps in pdf files
\usepackage{array} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{colortbl}
\definecolor{kugray5}{RGB}{224,224,224}
\usepackage{graphics}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage[titletoc,title]{appendix}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\usepackage[top=5mm]{geometry}
\usepackage[ bibstyle=numeric,, maxnames=1, backend=bibtex]{biblatex}
\addbibresource{ml_project.bib}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Novel Supervised Learning Based Approach for block page Detection}

\author{
Arian Akhavan Niaki\\
College of Information and Computer Sciences\\
University of Massachusetts\\
140 Governors Dr., Amherst, MA 01003 \\
\texttt{arian@cs.umass.edu} \\
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}
Current supervised learning approaches in machine learning literature have mostly concentrated on a single aspect data such as images or text and develop models for text and image classification. However, combining these two types of data could improve performance in the case of missing data in either images or text. In this paper, we crawl the Alexa top 500 web pages and create a dataset of web page screenshots and HTML documents. We then design and implement a deep convolutional neural network (CNN) which takes both the image and text data in order to perform block page detection. We compare our results to an image classification CNN, a text classification CNN and heuristic based block page detection. We show that our model achieves a macro-averaged F1 score of $0.981$ which is an improvement to the text's $0.9456$ but slightly worse than the image CNN's score of $0.985$.
\end{abstract}
\section{Introduction}
Extensive work has been done in supervised and unsupervised learning on both image and text classification. However, to the best of our knowledge, there does not exist an approach that applies supervised learning on a dataset containing both images and textual data. This can be advantageous in the case where we either have missing data in the images or text. We see this case occurring in the field of Internet censorship and Tor discrimination where crawlers fail to fetch both the screenshot and the HTML document of web pages.\\
Internet censorship often manifests in the form of \emph{block pages} returned to users accessing content. These block pages vary in appearance and content depending on the country. 
Moreover, previous work has shown that the users of anonymity systems such as Tor, receive discrimination in multiple forms such as CAPTCHAs, interaction based discrimination, and block pages.~\cite{torabuse}\\
Researchers have made attempts to characterize Internet censorship by detecting block pages automatically~\cite{imc14_phillipa}. Nevertheless, their efforts have largely been heuristic based and have shortcomings in distinguishing server errors from block pages as we will reveal in this paper.\\
In this paper, In order to detect and classify block pages and accurately measure discrimination against users, we use a Selenium-based crawler~\cite{selenium} to fetch the Alexa top $500$ websites from $50$ different Tor exit relays. We then design and implement a deep neural network which leverages both images and the HTML text of the web pages in order to perform multiclass classification by detecting block pages, server errors, connection errors, and legitimate web pages. In order to measure the performance and accuracy of our proposed model, we perform three experiments comparing the precision, recall, F1 score and macro-averaged F1 score of our proposed model with a state-of-the-art image classification deep neural net, a text classification deep neural net separately and heuristic based block page detection. We show that our proposed model can achieve better performance compared to the text CNN but is slightly outperformed by the image CNN. 
\section{Related Work}
As there is no related work on block page detection using machine learning techniques, in this section, we describe related work in image and text classification and prior work done in block page detection using heuristic techniques.

\subsection{Image Classification}
One of the foundational papers in image classification is ImageNet~\cite{imagehinton}. The structure of our image classification deep neural network is inspired by this work. However, our proposed network also receives a text document as input while this work only concentrates on images.
The authors trained a large, deep convolutional neural network to classify $1.2$ million high-resolution images into $10$ different classes. In contrast, our network is designed to classify into $4$ different classes.
They implement their model using parallel GPUs and show that the best number of layers for their task is five convolutional layers followed by max-pooling layers and three fully-connected layers. Another advancements in this paper is applying Rectified Linear Units (ReLUs) as the linear activation unit in their neural network which makes training six times faster than the alternative $f(x) = tanh(x)$. Their model's architecture is shown in Figure~\ref{fig:imagenet}.
\begin{figure}
\centering
        \includegraphics[totalheight=5cm]{imagenet.png}
    \caption{Imagenet architecture \protect\cite{imagehinton}}
    \label{fig:imagenet}
\end{figure}
They solve overfitting by using (1) data augmentation which artificially enlarges the dataset using transformations on images and (2) applying dropout where they set the output of each hidden neuron to zero with $0.5$ probability. These dropped neurons do not contribute to either forward pass or backpropagation allowing the model to learn more robust features.
One weakness of this paper is not declaring the decision behind the parameters of the neural network architecture. They also do not provide numerical results for the effects of different dropout probabilities. Altogether, this deep neural network would not work with our dataset. Our work complements this by adding a neural network for text classification as well and we hypothesize that it will perform better in the case where image data is missing but we have the HTML of the web page.

The authors of~\cite{nipsandrewng} have introduced an extension to convolutional neural networks (CNN) called Tiled CNNs. Rather than all weights being equal, these networks only require hidden units positioned $k$ steps of each other to have equal weights which will lead to learning more complex range of invariances. 
They also use an unsupervised learning algorithm that learns features from unlabeled image patches called Topographic ICA (TICA). They show that TICA pre-training for Tiled CNNs performs well on object recognition. We explore how this work can be integrated into our proposed model in section~\ref{sec:discussion}.

Authors in~\cite{icml_unsupervised} show that one can build high-level, class-specific feature detectors from unlabeled data in an inexpensive way. This is an extension to autoencoders which are used for learning low-level features. They use a large dataset of $200\times200$ images and feed it into a deep autoencoder with pooling and local contrast normalization. They employ local receptive fields,  local contrast normalization, and local $l_2$ pooling to scale for larger images and learn invariant features. Although our dataset contains only labeled data, we can use unlabeled data to learn high-level features and then train our network using labeled data. We will discuss how we can integrate this work with ours in section~\ref{sec:discussion}.

\subsection{Text Classification}
\begin{figure}
\centering
        \includegraphics[totalheight=3.5cm]{textnet.png}
    \caption{Architecture of text classification CNN \protect\cite{convtext}}
    \label{fig:textnet}
\end{figure}
There exists a myriad of publications on text classification and they all aim to assign free-text documents to predefined categories. In~\cite{convtext}, authors demonstrate that applying a CNN on top of a pre-trained word vector performs well on sentence-level classification tasks. They train a CNN with a single convolutional layer on top of word vectors that are pre-trained on $100$ billion words of Google News\footnote{https://code.google.com/p/word2vec/}. The results suggest that pre-trained vectors are universal feature extractors. Their model's architecture is shown in Figure~\ref{fig:textnet}. The structure of our text CNN is inspired by this work. They also apply regularization by using dropout on the output of the max pooling layer. One of the strong points in this work is performing experiments using several variants of the model. For example, comparing the result of a CNN with static pre-trained vectors, and CNN with randomly initialized word vectors on multiple datasets. However, this model also fails to work on our dataset because we have images in addition to HTML documents. Additionally, this model will not perform well on text documents which contain words not available in the embedding matrix. We complement this model by adding a neural network for image classification which will help in the cases where the text is missing in our data.
%In their model a sentence of length $n$ is represented as: $x_{1:n} = x_1 \oplus x_2 \oplus ... \oplus x_n$ and $x_i$ is the k-dimensional word vector for the i-th word. They apply a convolutional operation to a window of $h$ words to produce a new feature. $c_i = f(w\cdot x_{i:i+h-1} + b)$ where $f$ is a non-linear function. Therefore a feature map $c = [c_1, c_2, ...,c_{n-h+1}]$ is constructed, after this a max pooling operation is applied over this feature map where it outputs $\hat{c} = max(c)$ capturing the most important features for each feature map. These features are then passed to a fully-connected  layer. The output of the softmax layers is the probability distribution over labels. The model uses multiple filters with different window sizes to obtain multiple features. Regularization is applied by using dropout on the output of the max pooling layer. One of their strong points is performing experiments using several variants of the model. For example comparing the result of CNN with static pre-trained vectors, and CNN with randomly initialized words vectors on multiple datasets. This deep neural network would not also work with our dataset because it contains text in addition to images. Our work complements this by adding a neural network for image classification as well and will presumably perform better in the case where text data is missing but we have the image of the web page.
%

Character-level CNNs for text classification is studied in~\cite{nips_text}. They consider text as a raw signal at the character level and apply one-dimensional CNNs to it. An alphabet of size $m$ is constructed and each character is encoded to a one-hot vector with a fixed length $l_0$. Characters not present in the alphabet are set to all-zero vectors. One strong aspect of this paper is applying data augmentation using a thesaurus and replacing words by their synonyms to control generalization error. They compare their results to word-based CNN approach described in~\cite{convtext} as well as traditional text classification models such as bag-of-means on word embeddings. They conclude that the model's performance depends on various factors such as dataset size and choice of the alphabet and show that character-level CNNs have the potential of performing text classification without needing words.

Finally, a work done by~\cite{nn_survery} performs a survey on neural network models from the perspective of natural language processing. Specifically related to our work is their discussion on convolutional neural networks and the use of word embeddings for representing each feature as a vector in a low dimensional space. According to~\cite{nn_survery}, convolutional networks with pooling layers perform well in classification tasks. The strong point of this work that it does a comprehensive study on different aspects of neural network models for natural language processing. However, it does not provide sufficient details about each approach. We have designed our text classification neural network while having in mind that CNNs are to be integrated into larger networks in order to be effective~\cite{goldberg2017neural}.
\subsection{Block page detection}
The Authors in~\parencite{imc14_phillipa} study several metrics for block page detection such length difference, cosine and Document Object Model(DOM) similarity. They conclude that a length difference of more than $30.19\%$ between a censored page and its uncensored counterpart is the most accurate heuristic, relying on the idea that block pages are usually shorter than the original page. However, this approach fails to distinguish between server errors, block pages, and connection errors. Whereas, our model is designed to distinguish them accurately.
We did not find research on block page detection in the field of machine learning, however, machine learning techniques have been used for detecting website defacements by~\cite{meerkat}. The structure of their deep neural network was inspired by work from~\cite{imagehinton,nipsandrewng,icml_unsupervised}.

\section{Methodology}
In this section, we describe the architecture of the components in our proposed deep convolutional neural network for classifying web pages into $4$ classes. Our model consists of $2$ CNNs, one for images and one for text. The novelty of our approach is that we consider both data cases of image and text at the same time. This will prove advantageous when having missing data, either not having the HTML document or not having the screenshot. We have such data cases in our dataset where an error occurred and the HTML document is blank but we have a screenshot of the error page. Our proposed neural network is implemented in Python using the tensorflow library~\cite{tensorflow2015}. Another case where our proposed model will be superior to previous text classification models is that when web pages contain words that are not in the pre-trained word vector matrix. In these scenarios, our model uses the screenshots to make the prediction whereas a simple text classification CNN would fail.

\subsection{Image Neural Network}
\label{sec:imagecnn}
The design of our image classification convolutional neural network is inspired by ~\cite{imagehinton} and consists of $5$ layers. More specifically, the first three layers are convolutional layers and the final two are fully-connected layers. Since we have $4$ categories, the output of the last fully-connected layer is given as input to a $4$-way softmax function shown in equation~\ref{softmax} which predicts the class labels by producing a class probability $f^c(x,\theta) \in [0,1]$. Our proposed model uses softmax cross-entropy described in equation \ref{crossentropy} as the cost function.
\begin{equation} \label{crossentropy}
\begin{align}
& \theta_{*} = \argmin_\theta \sum_{n=1} ^{N} L_{ce}(y^c_{n},f^c(x_n,\theta))\\
& L_{ce}(y,p) = -y \times log(p)-(1-y)\times log(1-p)
\end{align}
\end{equation}

For optimizing the cost function we employ the Adam optimizer. According to empirical results, it achieves better results in comparison to other stochastic optimization methods~\cite{adam}. Our first convolutional layer filters the $160\times160\times3$ input image with $32$ kernels of size $16\times16\times3$ with a stride of $4$ pixels which means we slide the convolutional filter over the input image by $4$ pixels at a time, this will reduce the size of the CNN's output. Further, we want the output of the CNN to have the same size as the input, therefore we apply zero padding where $0$ values are added to the edge of the input to preserve the input size. ReLU is chosen as the activation function $f(x) = max(0,x)$ for the outputs of the CNN neurons because it is shown to be faster than the alternative $tanh$ activation function~\cite{imagehinton}. We then apply a $2\times2$ max-pooling where we select the largest value from each $2\times2$ window. 
The second and third layers, filter the output of their previous layer using $64$ kernels of size $8\times8\times3$ and $128$ kernels of size $4\times4\times3$ with a stride of $4$ pixels, $2\times2$ max pooling, and ReLU activation functions. In order to avoid overfitting, we apply dropout in our network as depicted in Figure~\ref{fig:CNN}. It has been shown in~\cite{CNNdropout2} that using dropout in convolutional neural networks improves performance. Prior work has also shown that using dropout on the last hidden layer of a fully-connected layer reduces the error rate~\cite{CNNdropout}. To be more concise, we set the output of each hidden neuron to zero with probability $0.5$. By doing this, we assure that the network is not always using the same neurons and is not getting fitted to the training data.
Finally, we flatten the output of the third convolutional layer in order to make it compatible as input to the fully-connected layers with $128$ neurons. If we want to run the network separately as we do in section~\ref{experiments}, the output of the last fully-connected layer will go through a $4$-way softmax function. The network's architecture is shown in the top part of Figure~\ref{fig:CNN}. The weights in the CNN and fully-connected layer are generated from a truncated normal distribution with $stddev = 0.05$ and the biases are set to $0.05$. The full network structure is available in the appendix.

\begin{equation}
  \label{softmax}
  P(y=c | x; \theta) = \frac{\exp(x^T \theta_c)}{\sum^{N}_{n=1} \exp(x^T \theta_n))},
\end{equation}

\subsection{Text Neural Network}
\label{sec:textcnn}
The design of our text classification CNN is inspired by~\cite{convtext}. Similar to our image classification CNN, it consists of $5$ layers, with the first three being convolutional layers and the rest being fully-connected layers. We need to apply some pre-processing on our HTML data in order to extract features before we can use them as input. First, we separate the section in which most of the content resides, which is the \textit{body} section, then we only consider the first $200$ words of the \textit{body}. Thus we will have the following vector for each HTML web page: $doc=[w_1,w_2,...w_{200}]$. In order to convert these words into features, we use pre-trained word vectors from GloVe's embedding matrix which contains 300-dimensional vectors for $1.9$M words. GloVe is a log-bilinear regression model of word representations~\cite{glove}. We fetch the vector $v_i$ for the first $200$ words in a HTML document from GloVe's embedding matrix and build $doc = [[v_1],[v_2],..[v_{200}]]$. In the case that the HTML document contains less than $200$ words, we pad the final $doc$ with $200$-dimensional zero vectors. Therefore, each HTML document will have the shape $1\times200\times300$ with $300$ being the number of channels. The structure of the text CNN is similar to the image classification one with the same cost function and optimization algorithm. The first convolutional layer applies a filter to the input using $128$ kernels of size $1\times4\times300$ with a stride of $3$ pixels. After applying the ReLU function we select the max value within a $1\times2$ window. The mathematical description is as follows:\\
Every document is represented by the concatenation of the word vectors $doc = [v_1 \oplus v_2 \oplus ... v_{200}]$ and by applying the convolution in windows of size $2$ and the ReLU activation function, we will have $c_i = ReLU(w\cdot v_{i:i+2} +b)$ so max pooling will compute the maximum between $c = [c_1, c_2, ... c_{199}]$ which is shown as $\hat{c} = max(c)$.
The second convolutional layer applies a filter using $64$ kernels of $1\times8\times300$ with the same max pooling scheme as the previous layer and the third convolutional layer applies a filter with $32$ kernels of $1\times16\times300$. The intuition behind increasing the layer sizes is that we first want look at the phrases and then move our way up to the whole sentence. ReLU is used as the activation function of all layers and we apply a dropout probability of $0.5$. The same flattening process occurs on the output of the third convolutional layer and we go through two fully-connected layers with dropout probability of $0.5$. As we will show in section~\ref{experiments} the output of the last fully-connected layer will go through a $4$-way softmax function if we want to run the network separately. The network's architecture is shown in the bottom part of Figure~\ref{fig:CNN}. The complete network structure is available in the appendix.
\begin{figure}
\centering
        \includegraphics[totalheight=3.2cm]{Combined}
    \caption{The architecture of our proposed deep convolutional neural network}
    \label{fig:CNN}
\end{figure}

\subsection{Proposed Neural Network}
  \label{ProposedNet}

Our network as depicted in Figure~\ref{fig:CNN} first receives the HTML document and the image of the web page. It then feeds the input into the two separate CNNs in parallel. If the HTML document is blank we feed a $1\times200\times300$ zero vector to the text classification CNN and the associating screenshot to the image classification CNN. There exist cases where we have the legitimate HTML document of the web page but the screenshot is a white picture, we proceed as before and feed each of the inputs to the appropriate CNN. Our model concatenates the output of each of the text and image CNNs and feeds
them as an input to another fully-connected layer where a dropout of $0.5$ is applied. Finally, we go through one last fully-connected layer. A $4$-way softmax function is applied to the output of the last fully-connected layer to predict the class label.

\section{Datasets} 
\label{datasets}
We use an automated Selenium-based web crawler~\cite{selenium} written in Python for collecting screenshots and the HTML documents of the Alexa\footnote{https://www.alexa.com/topsites} top $500$ web pages from a control host and 50 Tor~\cite{tor} exit relays located in $23$ countries. Our control host does not connect to Tor exit relays and is located in an uncensored network in the US, the uncensored web pages will be used in the heuristic based block page detection. Our crawls took place in November $2017$ and the dataset consists of $21246$ screenshots and their associated HTML documents. Glancing at the screenshots we can clearly see $4$ types of pages. (1) The Tor user can access the website like a non-Tor user which we see in Figure~\ref{fig:okpage}, we label these pages as \textit{ok}. (2) the Tor user reaches a server error page which we label with \textit{servererror} an example of these pages is shown in Figure~\ref{fig:servererror}. (3) the Tor user gets a connection failed or timeout page, we label these as \textit{connectionerror}. A sample connection error page is depicted in Figure~\ref{fig:connectionerror} and finally (4) the Tor user gets a block page or a page containing some form of CAPTCHA which we label as \textit{block} that can be seen in Figure~\ref{fig:blockpage}.
Our images have variable-resolutions, therefore we rescale our images to $160\times160$. We do not crop the center of each image because many of the server error pages have content shown in the top part of the page. No pre-processing or data augmentation is executed on our data. The raw RGB values of the pixels are used as our image input. For the HTML documents, we take out the \textit{body} section of the page and parse over the first $200$ words and provide this as our text inputs. This is because of the fact that the text content in an HTML document is available in the body section. Our labeled dataset consists of $17783$, $2175$, $769$ and $519$, ok, block, connection error and server error pages.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ok.png}
        \caption{Ok web page}
        \label{fig:okpage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{block.png}
        \caption{Block page}
        \label{fig:blockpage}
    \end{subfigure}
    \hfill
        \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{connectionerr.png}
        \caption{Connection error page}
        \label{fig:connectionerror}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{servererr.png}
        \caption{Server error page}
        \label{fig:servererror}
    \end{subfigure}
    \caption{Samples of our dataset}
    \label{fig:sample_dataset}
\end{figure}
\section{Experiments}
\label{experiments}
In order to evaluate the performance of our proposed model, we compare it with (1) a CNN for image classification as described in section~\ref{sec:imagecnn} (2) a CNN for text classification as described in section~\ref{sec:textcnn} and, (3) prior work for block page detection (heuristic based length difference) using a set of performance metrics. The heuristic based block page detection is a threshold that marks any difference in size over $30.19\%$ between a censored page and the uncensored version as blocked~\cite{imc14_phillipa}.

We conduct all the experiments on the previously mentioned dataset where we randomly take 80\% of the data as training and set aside the remaining 20\% for the test phase. We take out 25\% of the training data for validation. Our hypothesis is that the our designed CNN should perform better than the CNNs for image and text classification. The reason being that there are cases where either the image or the text is missing for a data case and our proposed CNN would be more beneficial than a single CNN only focusing on text or image. Finally, previous techniques for block page detection were only heuristic based and had false positives, our method is designed to classify web pages into $4$ distinct classes more accurately. In all experiments, the training batch size, validation batch size and test batch size are $607$, $85$ and $607$ respectively. We also apply an early stopping technique to our experiments. If the validation loss value does not improve in $7$ subsequent epochs we do not continue and report the performance metric values when running the test data on the model's best-learned parameters(weights).

\subsection{Performance Metrics}
Accuracy is not a valid performance metric for our model, because our dataset is unbalanced. Meaning that the number of \textit{ok} pages is much more than the other classes. Hence, we select precision, recall, F1 score and macro-average F1 score as our performance metrics. The formulas are also shown in equation~\ref{eqn:eqlabel} and~\ref{eqn:eqlabel2}. These performance metrics have been used to evaluate block page detection in prior work as well ~\cite{imc14_phillipa}. In this work, $tp_c$(True Positives) is the number of correctly predicted cases of class $c$, $fp_c$(False Positives) is the number of cases that are incorrectly predicted as class $c$ and $fn_c$(False Negatives) is the number of cases belonging to class $c$ that the classifier failed to classify. The macro-average F1 score is the average F1 score over all classes, this is important to us because our classes do not have the same number of instances and the macro-averaged F1 gives equal weight to all classes.
 \begin{align}
\label{eqn:eqlabel}
\begin{split}
 precision_c = \dfrac{tp_c}{tp_c+fp_c} , \quad 
recall_c = \dfrac{tp_c}{tp_c+fn_c}, \quad 
\end{split}
\end{align}


 \begin{align}
\label{eqn:eqlabel2}
\begin{split}
F1_c = 2 \cdot \dfrac{precision_c \cdot recall_c}{precision_c + recall_c} \quad
F1_{macro-averaged} = \dfrac{\sum_{c=1}^{C} F1_c}{C}
\end{split}
\end{align}



\subsection{Hyper-parameters and Training}
We perform a grid search for hyper-parameter tuning. More specifically, we take the learning rate ($\alpha$) and the structure of the fully-connected layers in the networks as our hyper-parameters. For the learning rate, we consider $0.001$ and $0.0001$ and for the fully-connected layers we consider: the case where we described our model architecture in section~\ref{ProposedNet} which we indicate with $(1)$ for the rest of this paper and the case where we change the output size of the first fully-connected layer after the flatten process to $256$ then we add another fully-connected layer with the output size of $128$, finally we pass through another fully-connected layer with output size $4$, we specify this case with $(2)$ for the rest of this paper. The alternative architecture is available in the appendix. Therefore, for each experiment we will consider the effect of these hyper-parameters on the model's performance. The results are shown in table~\ref{tab:hyperparam}.
\section{Results}
After applying hyper-parameter tuning on our approach and the image and text classification CNNs we can compare their results on the same test set for evaluating the experiments in section~\ref{experiments}. For detecting \textit{ok} pages and \textit{block pages}, the image CNN works best with $\alpha=0.0001$ and the alternative fully-connected layer design $(2)$, our proposed model performs best with $\alpha=0.0001$ and the text CNN is optimized with $\alpha=0.0001$ and the default architecture $(1)$. The highest F1 score for each class is specified with a bold font in table~\ref{tab:hyperparam}.
\begin{figure*}
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{okF1.png}
            \caption[]%
            {{\small Ok class}}    
            \label{fig:okF1}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{blockF1.png}
            \caption[]%
            {{\small Block page class}}
            \label{fig:blockF1}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{connectionF1.png}
            \caption[]%
            {{\small Connection error class}}    
            \label{fig:connectionF1}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{ServerF1.png}
            \caption[]%
            {{\small Server error class}}    
            \label{fig:serverF1}
        \end{subfigure}
        \caption[ F1 score vs number of epoch for each category ]
        {\small F1 score vs number of epoch for each category} 
        \label{fig:F1epoch}
        \vspace{-0.4cm}

    \end{figure*}
For evaluating our experiments, we train each of the models, namely the image CNN, text CNN and our proposed CNN with their optimized hyper-parameters and compare the F1 score of each of the $4$ classes and the macro-averaged F1 score for all the classes. For the \textit{ok} and \textit{block page} classes, our proposed model receives a $0.998$ and $0.99$ F1 score, which is higher than the text CNN and is the same as the image CNN but with fewer epochs. The heuristic based method achieves a $0.80$ and $0.47$ F1 score for the \textit{ok} and \textit{block} classes, which is a clear indication of it being out-of-date. For the \textit{server error page} class, all models perform the same but our proposed model has the least number of epochs by $35$. Finally, for the \textit{connection error} pages, the Image CNN seems to outperform the other two. The macro-average F1 score for the Image CNN, text CNN and our proposed model are $0.985$, $0.9456$ and $0.981$ respectively. This shows that the Image CNN outperforms the two other models and our proposed model performs close to the image CNN and better than the text CNN. However, this might be because of the fact that our dataset does not have many missing images of web pages. We can not come to a complete conclusion before analyzing on a larger dataset.
Figure~\ref{fig:F1epoch} plots the F1 score of each class vs the number of epochs each model had to train in order to obtain the results. Since we have more instances for the \textit{ok} class, we see that in Figure~\ref{fig:okF1} the starting F1 score is close tos $0.85$ which is relatively high. Whereas, for the other classes the F1 score starts from $0$.
In order to ensure that the training converges without overfitting, we have plotted the training and validation loss values of our $3$ experiments for the image CNN, text CNN and our proposed model which can be seen in Figure~\ref{fig:validationloss}. We see no signs of overfitting as the validation loss curve closely follows the training loss curve.

   
\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{1}{|>{{}}c|}{}&\multicolumn{3}{c|}{Ok page}&\multicolumn{3}{c|}{Block page}&\multicolumn{3}{c|}{Server error page}&\multicolumn{3}{c|}{Connection error page}&\multicolumn{1}{c|}{Epoch}\\
    \arrayrulecolor{}
    \arrayrulecolor{black}
    \cline{2-4}     \cline{5-7} \cline{8-10}\cline{11-13}

    \multicolumn{1}{|>{}c|}{Model}&Precision&Recall&F1&Precision&Recall&F1&Precision&Recall&F1&Precision&Recall&F1& \\
    \hline
    Image (1) $\alpha = 0.001$ &0.997&0.998&0.998&0.993&0.988&0.99&0.98&0.962&\bf{0.971}&0.981&0.981&0.981&92\\
    \hline
         Image (2)  $\alpha =0.001$&0.996&0.998&0.997&0.986&0.979&0.982&0.971&0.962&0.966&0.993&0.981&\bf{0.987}&85\\
    \hline
     Image (1)   $\alpha =0.0001$&0.99&0.99&0.99&0.988&0.97&0.979&0.953&0.962&0.957&0.993&0.981&0.987&91\\
    \hline
    Image (2)    $\alpha =0.0001$&0.998&0.998&\bf{0.998}&0.993&0.988&\bf{0.99}&0.935&0.962&0.948&0.987&0.981&0.984&115\\
    \hline
        HTML (1) $\alpha =0.001$ &0.981&0.999&0.99&0.995&0.935&0.964&0.98&0.962&0.971&0.991&0.759&0.86&59\\
    \hline
         HTML (2)  $\alpha =0.001$&0.982&1.0&0.99&0.997&0.938&0.96&0.98&0.96&\bf{0.971}&0.99&0.759&\bf{0.86}&95\\
    \hline
     HTML (1)   $\alpha =0.0001$&0.982&1.0&\bf{0.99}&1.0&0.938&\bf{0.968}&0.971&0.962&0.966&0.991&0.759&0.86&125\\
    \hline
    HTML (2)    $\alpha =0.0001$&0.981&0.999&0.99&0.997&0.935&0.965&0.971&0.962&0.966&0.991&0.759&0.86&105\\
    \hline
            Our model (1) $\alpha =0.001$ &0.995&0.999&0.997&0.995&0.983&0.99&0.98&0.962&\bf{0.971}&0.98&0.95&0.965&57\\
    \hline
            Our model (2)  $\alpha =0.001$&0.997&0.998&0.998&0.995&0.965&0.0.98&0.962&0.971&0.967&0.939&0.981&0.959&98\\
    \hline
            Our model (1)   $\alpha =0.0001$&0.998&0.999&\bf{0.998}&0.997&0.983&\bf{0.99}&0.971&0.952&0.962&0.968&0.981&\bf{0.974}&92\\
    \hline
            Our model (2)    $\alpha =0.0001$&0.997&0.999&0.998&0.997&0.933&0.964&0.971&0.962&0.966&0.829&0.987&0.90&67\\
    \hline
             Length difference    $\alpha =0.0001$&0.93&0.71&0.80&0.31&1.0&0.47&0&0&0&0&0&0&0\\
    \hline
  \end{tabular}
  }
  \caption{Performance metrics in various scenarios}
  \label{tab:hyperparam}
\end{table}
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{html_val_train.png}
        \caption{Text CNN}
        \label{fig:html_loss}
    \end{subfigure}
    \hfill
        \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{png_val_train.png}
        \caption{Image CNN}
        \label{fig:image_loss}
    \end{subfigure}
   \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{model_val_train.png}
        \caption{Proposed Model}
        \label{fig:model_loss}
    \end{subfigure}
    \caption{Training and validation loss curves for the best settings of our $3$ experiments}
    \label{fig:validationloss}
\end{figure}

 
\section{Discussion and Conclusions}
\label{sec:discussion}
In this work, we design and implement a novel deep convolutional neural network which works with text and images simultaneously. We apply our model to the task of block page detection. We collect a dataset of web page crawls consisting of $21246$ screenshots and HTML documents of the Alexa top 500 web pages from 50 Tor exit relays. We then study and compare the performance of our proposed model on our dataset, with an image convolutional neural network that works only with images, a textual convolutional neural network which only works with text and previous work on block page detection using heuristics. Our hypothesis is that our designed model will work better in cases when there are missing data in either the text or the screenshots. We use the F1 score and the macro-average F1 score as a performance metric. Our results prove that the previous techniques on block page detection are no longer functional and our proposed model performs remarkably better. However, our model performs slightly worse than the image CNN but requires fewer epochs. In conclusion, a larger dataset is required to evaluate the true performance of our proposed model with sufficient missing data cases.

\subsection{Future work}
As a future work, our proposed model can be extended by using stacked autoencoders with unlabeled data in order to learn high-level features~\cite{icml_unsupervised} before we start the training phase. Furthermore, we plan to investigate the effect of using data augmentation in order to avoid overfitting. Data augmentation is used to enlarge the dataset by transforming the images and using a thesaurus to replace words with their synonyms~\cite{icml_unsupervised, nips_text}. We also plan to evaluate the model on larger datasets.

%
%
%
%\subsubsection*{References}
%
%References follow the acknowledgments. Use unnumbered third level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to `small' (9-point) 
%when listing the references. {\bf Remember that this year you can use
%a ninth page as long as it contains \emph{only} cited references.}
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.
%
%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.
%
%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}
\printbibliography
\clearpage
\begin{appendices}

\section{Image CNN}
    \begin{figure}[!htb]
\centering
        \includegraphics[totalheight=2cm]{CNN_PNG}
    \caption{Image classification neural network}
    \label{fig:CNN_PNG}
\end{figure}

\section{Text CNN}

    \begin{figure}[!htb]
\centering
        \includegraphics[totalheight=2cm]{HTML_PNG}
    \caption{Text classification neural network}
    \label{fig:HTML_PNG}
\end{figure}

\section{Alternative Combined CNN (2)}
    \begin{figure}[!htb]
\centering
        \includegraphics[totalheight=3cm]{Combined2}
    \caption{Alternative proposed model neural network}
    \label{fig:Combined_2}
\end{figure}


\end{appendices}

\end{document}
